{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "!pip install torch pandas numpy joblib tqdm sklearn "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Requirement already satisfied: torch in /Users/m20027/Library/Python/3.9/lib/python/site-packages (1.9.0)\n",
      "Requirement already satisfied: pandas in /Users/m20027/Library/Python/3.9/lib/python/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy in /Users/m20027/Library/Python/3.9/lib/python/site-packages (1.21.2)\n",
      "Requirement already satisfied: joblib in /Users/m20027/Library/Python/3.9/lib/python/site-packages (1.0.1)\n",
      "Requirement already satisfied: tqdm in /Users/m20027/Library/Python/3.9/lib/python/site-packages (4.62.2)\n",
      "Requirement already satisfied: sklearn in /Users/m20027/Library/Python/3.9/lib/python/site-packages (0.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/m20027/Library/Python/3.9/lib/python/site-packages (from torch) (3.10.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/m20027/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/m20027/Library/Python/3.9/lib/python/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/site-packages (from sklearn) (1.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/m20027/Library/Python/3.9/lib/python/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/m20027/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sklearn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/m20027/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sklearn) (1.7.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import skimage.io\n",
    "import skimage.metrics\n",
    "#from dataset_and_issue import dataset, issue\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "#import KFold, train_test_split\n",
    "#import MinMaxScaler, StandardScaler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Transformer:\n",
    "    def __init__(self, is_train=False):\n",
    "        self.transformer = dict()\n",
    "        self.is_train = is_train\n",
    "        if not self.is_train:\n",
    "            self.transformer = self.load()\n",
    "\n",
    "    def __call__(self, df, metric):\n",
    "        if self.is_train:\n",
    "            return self.fit_transform(df, metric)\n",
    "        else:\n",
    "            df[:] = self.transformer[metric].transform(df)\n",
    "            return df\n",
    "\n",
    "    def inverse_transform(self, scaled, metric):\n",
    "        return self.transformer[metric].inverse_transform(scaled)\n",
    "\n",
    "    def fit_transform(self, df, metric):\n",
    "        self.transformer[metric] = MinMaxScaler()\n",
    "        df = self.transformer[metric].fit_transform(df)\n",
    "        self.dump()\n",
    "        return df\n",
    "\n",
    "    def dump(self, filename='/tmp/transformer.bin'):\n",
    "        with open(filename, 'wb') as f:\n",
    "            joblib.dump(self.transformer, f)\n",
    "\n",
    "    def load(self, filename='/tmp/transformer.bin'):\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = joblib.load(f)\n",
    "        return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class SimilarityDataset(Dataset):\n",
    "    def __init__(self, path, sequence_length, device=None, transform=Transformer, is_train=True):#学習じはtrue\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = Transformer\n",
    "\n",
    "        metrics        = [\"psnr\", \"ssim\"]\n",
    "        target_metrics = [\"throughput\", \"loss_rate\"]#教師deta\n",
    "\n",
    "        self.input_dim  = len(metrics)\n",
    "        self.target_dim = len(target_metrics)\n",
    "\n",
    "        if is_train:\n",
    "            files = glob.glob(os.path.join(path, \"*.json\"))#pathの定義\n",
    "            df = self.read_files(files, self.sequence_length)#ファイルの読み込み\n",
    "        else:\n",
    "            df = self.read_file(path, sequence_length)#ファイル群の情報，シーケンス\n",
    "\n",
    "        if self.transform:\n",
    "            for metric in metrics:\n",
    "                df[[metric]] = self.transform(df[[metric]], metric)\n",
    "\n",
    "        df.index = df[[\"video_type\", \"throughput\", \"loss_rate\", \"interval\"]]#データのインデックス情報\n",
    "        indices = df[[\"video_type\", \"throughput\", \"loss_rate\", \"interval\"]].index.unique()\n",
    "        grouped_df = df.groupby([\"video_type\", \"throughput\", \"loss_rate\", \"interval\"])#テーブルデータの集約\n",
    "\n",
    "        self.data = []#\n",
    "        self.target = []#\n",
    "        for index in tqdm(indices):#学習データの生成（1000フレーム（シリーズに格納）ずつ）\n",
    "            series = grouped_df.get_group(index)#\n",
    "            if len(series) < self.sequence_length:#\n",
    "                continue\n",
    "            video_type, throughput, loss_rate, interval = index#\n",
    "            self.data.append(series[metrics].values)#情報の抜き出し\n",
    "            self.target.append([throughput, loss_rate])#情報の抜き出し\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ret = torch.tensor(self.data[idx], dtype=torch.float64, device=self.device)\n",
    "        trg = torch.tensor(self.target[idx], dtype=torch.float64, device=self.device)\n",
    "        return ret, trg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def read_files(self, files, sequence_length):\n",
    "        li_df = []\n",
    "        for filename in tqdm(files):\n",
    "            df = self.read_file(filename, sequence_length)\n",
    "            li_df.append(df)\n",
    "        df = pd.concat(li_df)\n",
    "        return df\n",
    "\n",
    "    def read_file(self, filename, sequence_length):\n",
    "        with open(filename) as f:\n",
    "            d = json.load(f)\n",
    "        df = pd.DataFrame.from_dict(d)\n",
    "        df[\"interval\"] = df[\"frame_index\"] // sequence_length#データの番号付けのためにわる\n",
    "        df = df.sort_values(\"frame_index\")\n",
    "        return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def collate_fn(batch):\n",
    "    data, targets = list(zip(*batch))\n",
    "    data    = torch.stack(data)\n",
    "    targets = torch.stack(targets)\n",
    "    return data, targets\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class NetworkStateEstimationFromVideoStreaming(nn.Module):\n",
    "    def __init__(self, input_dim, target_dim, sequence_length, hidden_dim):\n",
    "        super(NetworkStateEstimationFromVideoStreaming, self).__init__()\n",
    "        self.input_dim       = input_dim\n",
    "        self.target_dim      = target_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.hidden_dim      = hidden_dim\n",
    "\n",
    "        self.fc1     = nn.Linear(self.input_dim * self.sequence_length, self.hidden_dim)\n",
    "        self.fc2     = nn.Linear(self.hidden_dim, self.target_dim)\n",
    "        self.relu    = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim * self.sequence_length)\n",
    "        out = self.fc1(x)#層の定義\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train(input_dir=\"/content/drive/MyDrive/similarity_measures/train\", model_dir=\"/content/models\", seed=1, fold=4, batchsize=32, log_seq=1):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    transformer = Transformer(is_train=True)#学習に関する関数，input_dirはインプットする引数\n",
    "\n",
    "    similarity_dir = input_dir#\n",
    "    model_dir      = model_dir#\n",
    "    seed           = seed#シード値\n",
    "    fold_split     = fold#K分割交差検証\n",
    "    batch_size     = batchsize#\n",
    "\n",
    "    set_seed(seed)#\n",
    "\n",
    "    sequence_length = 1000#\n",
    "    hidden_dim      = 130#隠れ層の次元\n",
    "    num_layers      = 2#\n",
    "    max_epoches     = 100000#最大のエフォック回数\n",
    "    log_seq         = log_seq#\n",
    "\n",
    "    train_and_valid_dataset = SimilarityDataset(#データセット読み込み\n",
    "            path=similarity_dir,\n",
    "            sequence_length=sequence_length,\n",
    "            device=device,\n",
    "            transform=transformer,\n",
    "            )\n",
    "    if not os.path.exists(model_dir):#\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    X = train_and_valid_dataset.data#訓練用データ\n",
    "    Y = train_and_valid_dataset.target#教師用データ\n",
    "    skf = KFold(n_splits=fold_split, shuffle=True, random_state=seed)#分割，シャッフル，乱数\n",
    "    for i, (train_idx, valid_idx) in enumerate(skf.split(X, Y)):\n",
    "        train_dataset = Subset(train_and_valid_dataset, train_idx)\n",
    "        valid_dataset = Subset(train_and_valid_dataset, valid_idx)\n",
    "        train_size = len(train_dataset)#サイズの出力\n",
    "        valid_size = len(valid_dataset)\n",
    "        print(f'fold : {i+1} train dataset size : {train_size} valid dataset size: {valid_size}')\n",
    "\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)#学習用：読み込むデータセットの定義，シャッフル，ミニバッチ等\n",
    "        valid_dataloader = DataLoader(valid_dataset, batch_size=valid_size, shuffle=True, collate_fn=collate_fn)#検証用\n",
    "\n",
    "        input_dim  = train_and_valid_dataset.input_dim\n",
    "        target_dim = train_and_valid_dataset.target_dim\n",
    "\n",
    "        model = NetworkStateEstimationFromVideoStreaming(input_dim, target_dim, sequence_length, hidden_dim).to(device)#モデル定義\n",
    "        \n",
    "        loss_function = nn.SmoothL1Loss()#ロス関数\n",
    "        l1loss_function = nn.L1Loss()\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "        for epoch in range(1, max_epoches + 1):#学習のフレーズ\n",
    "            train_loss = 0\n",
    "            for train_inputs, train_targets in train_dataloader:#データ取得学習用，ターゲットの順番\n",
    "\n",
    "                train_inputs  = train_inputs.float()\n",
    "                train_targets = train_targets.float()\n",
    "                train_inputs  = train_inputs.to(device)\n",
    "                train_targets = train_targets.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                train_scores = model(train_inputs)#推論計算\n",
    "\n",
    "                loss = loss_function(train_scores, train_targets)\n",
    "                loss.backward()#勾配情報\n",
    "                optimizer.step()#パラメータ\n",
    "\n",
    "                train_loss += loss.item()\n",
    "            train_loss /= len(train_dataloader)\n",
    "\n",
    "            with torch.no_grad():#検証\n",
    "                valid_inputs, valid_targets = iter(valid_dataloader).next()\n",
    "\n",
    "                valid_inputs  = valid_inputs.float()#入力\n",
    "                valid_targets = valid_targets.float()\n",
    "                valid_inputs  = valid_inputs.to(device)\n",
    "                valid_targets = valid_targets.to(device)\n",
    "\n",
    "                valid_scores = model(valid_inputs)#モデルに対して情報を与える\n",
    "\n",
    "                loss = loss_function(valid_scores, valid_targets)\n",
    "                valid_loss = loss.item() / len(valid_dataloader)\n",
    "\n",
    "                val_scores  = valid_scores.to('cpu').detach().numpy().astype(np.float32)\n",
    "                val_targets = valid_targets.to('cpu').detach().numpy().astype(np.float32)\n",
    "                throughput_scores = valid_scores[:, 0]\n",
    "                loss_rate_scores  = valid_scores[:, 1]\n",
    "                throughput_targets = valid_targets[:, 0]\n",
    "                loss_rate_targets  = valid_targets[:, 1]\n",
    "\n",
    "                throughput_loss, loss_rate_loss = (l1loss_function(throughput_scores, throughput_targets).item(), l1loss_function(loss_rate_scores, loss_rate_targets).item())\n",
    "\n",
    "            if epoch < 10 or epoch % log_seq == 0:\n",
    "                print(f\"Epoch: [{epoch}/{max_epoches}] train/valid loss: {train_loss:.4f} / {valid_loss:.4f} throughput/loss rate: {throughput_loss:.4f} / {loss_rate_loss:.4f}\")\n",
    "\n",
    "            if epoch % 500 == 0:\n",
    "                torch.save(model.state_dict(), f\"{model_dir}/fold{i + 1}_{epoch}.mdl\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def test(input_path=\"dataset_and_issue/dataset/received\", model_path=\"/content/models/fold1_1000.mdl\", seed=1):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    similarity_path = input_path\n",
    "    model_path     = model_path\n",
    "    seed           = seed\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    sequence_length = 1000\n",
    "    hidden_dim      = 130\n",
    "    num_layers      = 2\n",
    "    max_epoches     = 1000\n",
    "\n",
    "    test_dataset = SimilarityDataset(\n",
    "            path=similarity_path,\n",
    "            sequence_length=sequence_length,\n",
    "            device=device,\n",
    "            transform=Transformer,\n",
    "            is_train=False\n",
    "            )\n",
    "\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    input_dim  = test_dataset.input_dim\n",
    "    target_dim = test_dataset.target_dim\n",
    "\n",
    "    model = NetworkStateEstimationFromVideoStreaming(input_dim, target_dim, sequence_length, hidden_dim).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    loss_function = nn.L1Loss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_inputs, test_targets = iter(test_dataloader).next()\n",
    "        test_inputs  = test_inputs.float()\n",
    "        test_targets = test_targets.float()\n",
    "\n",
    "        test_scores = model(test_inputs)\n",
    "        test_targets = test_targets.to(device)\n",
    "        test_loss   = loss_function(test_scores, test_targets)\n",
    "\n",
    "        throughput_scores = test_scores[:, 0]\n",
    "        loss_rate_scores  = test_scores[:, 1]\n",
    "        throughput_targets = test_targets[:, 0]\n",
    "        loss_rate_targets  = test_targets[:, 1]\n",
    "\n",
    "        throughput = throughput_scores.mean()\n",
    "        loss_rate  = loss_rate_scores.mean()\n",
    "\n",
    "        throughput_loss, loss_rate_loss = (loss_function(throughput_scores, throughput_targets).item(), loss_function(loss_rate_scores, loss_rate_targets).item())\n",
    "        print(f\"model: {model_path} throughput / loss rate {throughput} / {loss_rate} throughput loss/ loss rate loss: {throughput_loss:.4f} / {loss_rate_loss:.4f}\")z"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train(log_seq=50)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}